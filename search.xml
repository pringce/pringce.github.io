<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[hexo博客如何迁移至其它电脑]]></title>
    <url>%2F2019%2F07%2F20%2Fhexo%E5%8D%9A%E5%AE%A2%E5%A6%82%E4%BD%95%E8%BF%81%E7%A7%BB%E8%87%B3%E5%85%B6%E5%AE%83%E7%94%B5%E8%84%91%2F</url>
    <content type="text"><![CDATA[​ 当你在一台电脑上写博客写的蛮爽的，但突然有一天你的电脑坏了或者被偷了咋办？你那些本地的博客可咋整！一想到这种有可能发生的危险，我就夜不能寐呀。于是我就开始疯狂地查找资料，但网上的资料良莠不齐。于是本人就自己慢慢的摸索嘛，这是一个程序员最基本的素质(其实还是自己太菜了)。下面我就把我的总结写一下，希望能对以后的自己和大家有帮助。 1、在新电脑上安装NodeJs和Git​ 这一步很简单，具体怎么安装，可以参见我的上一篇博文：利用hexo框架搭建个人博客(手把手教学) 2、GitHub新建分支​ 在pringce.github.io仓库下： 这里的分支名可以任意设置。 3、设置分支为默认仓库当前仓库-&gt;Settings-&gt;Branches-&gt;Default Branch 4、clone至本地​ 将该分支克隆至本地 1git clone https://github.com/pringce/pringce.github.io.git ​ cd进入 clone 下来的pringce.github.io文件夹，在此文件夹目录下git bash执行git branch命令，应该是新建的分支名 new。 5、新电脑生成ssh key并添加至GitHub​ 依次按照下述步骤生成本机ssh key： 1git config --global user.email "GitHub注册并验证时的邮箱" 1git config --global user.name "GitHub用户名" 1ssh-keygen // 一直按enter就行 在本机目录“C:\Users\10530\.ssh”会生成两个文件：id_rsa(私钥)和id_rsa.pub(公钥)。打开id_rsa.pub，复制里面的内容到GitHub: Settings -&gt; SSH and GPG keys -&gt; (填写)Title -&gt; (粘贴)Key -&gt; Add SSH Key 测试是否成功： 1ssh -T git@github.com 输出 You’ve successfully authenticated 表示添加key 成功。 6、安装hexo1npm install hexo-cli -g 7、将原博客文件拷贝至pringce.github.io文件夹​ 将原博客文件夹下的文件拷贝至pringce.github.io，需要拷贝6个：_config.yml（站点配置文件）、themes\（主题）、source\（博客源文件）、scaffolds\（文章的模板）、package.json、.gitignore。 此时，我们不需要再用hexo init来生成博客了，因为需要的文件我们已经拷贝过来了。 8、提交本地至new分支​ 进入pringce.github.io文件夹，依次执行 1git add . 1git commit -m &quot;新电脑&quot; 1git push 这样，master分支用于保存博客静态资源，提供博客页面供人访问；new分支用于备份博客部署文件，供自己修改和更新，两者在一个GitHub仓库内互不冲突。 9、安装hexo依赖的包1npm install ​ 所依赖的包都在上一步中的package.json备份文件里，所以直接这一个命令就可以了。就可以把你以前配置的那些包都安装上了。 10、新旧电脑更新博客​ 依次按照以下步骤来更新博客： 1git pull 1hexo new &quot;name&quot; 1hexo clean 1hexo g 1hexo s(测试时候在localhost看一下) 1hexo d(部署) 1git add . 1git commit -m &quot;新电脑&quot; 1git push //保证new分支版本为最新 因此，如果我们电脑又坏了或者又被偷了。。。那我们可以先获得新电脑的ssh key，然后添加到GitHub。然后把new分支下文件git clone下来，然后再新建、编辑、部署一步一步来即可。 好啦，心理的最大隐患解除了，可以安心睡觉啦。大家晚安！！]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>个人博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树全解析]]></title>
    <url>%2F2019%2F07%2F20%2F%E5%86%B3%E7%AD%96%E6%A0%91%E5%85%A8%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言​ 其实大家都说决策树简单，但个人觉得如果想学好这个算法其实还是需要下一些功夫的。决策树是一种基本的分类与回归方法(不要觉得决策树只能分类哦，它也可以做回归的)。 ​ 为了方便下面的讨论，我们先做一下说明： ​ 假设有一组训练数据 D=(x1,y1),(x2,y2),\cdots,(xn,yn)​ 表示有n个样本，样本共分为K类情况下，yi的取值来自K个类别值 (C1,C2,\cdots,Ck) ​ 另外，无论是ID3树、C4.5树还是CART树都是差不多的，都由特征选择(分裂特征选择算法)、树的生成和剪枝组成。它们之间的区别主要是特征选择和剪枝算法不同，这些我们后面会详细讨论，这里先留个伏笔。 ID3树​ 在介绍ID3决策树之前，我想先说一个概念：信息增益。 ​ 一个离散型随机变量x的概率分布为 P(x=x_i)=p_i​ ，则x的信息熵定义为 H(x)=-\sum_{i=1}^{n}{p_i}​ 数据集的熵表征其类别的不确定程度，而数据集关于某个特征的条件熵则表征着给定某个特征后，其类别的不确定程度： 数据集的熵—条件熵 = 该特征使数据集不确定性减小程度 设数据集D的熵为H(D)，关于特征Am的条件熵为H(D|Am)，因此可以获得信息增益为 g(D|A_m)=H(D)-H(D|A_m)其中， H(D)=-\sum_{k=1}^{K}{\frac{|C_k|}{|D|}}log_2\frac{|C_k|}{|D_k|} H(D|A_m)=\sum_{i=1}^{K}{\frac{|D_i|}{|D|}}H(D_i)​ Di 表示训练样本中特征A取值为 ai 的样本点集合(比如说特征A有三个可能的取值，则K为3)。 ID3决策树构建​ 自根节点开始，选择信息增益最大的特征作为节点的分类特征，并根据该特征的可能取值将训练数据分配到不同的子节点(对子节点进行同样的操作)。 ​ 若子节点的所有样本属于同一类别或该子节点处所有特征的信息增益均小于给定阈值或无可供选择的特征，此时我们判定该子节点为一个叶节点，将该叶节点的样本数量最多的类别作为该叶节点的类别。 ​ 算法步骤如下： 若D中样本特征为空，那么树T为一棵单节点树，将样本数量最大的类别作为节点类别，返回T。否则转到第二步； 计算D关于所有特征的信息增益，若信息增益均小于某一阈值，则T为单节点树。否则转到第三步； 选择信息增益最大的特征作为分裂特征，依据该分裂特征的所有可能取值建立相应子节点。若子节点样本全为同一类别，则子节点为叶节点。若均为叶节点，返回T。否则转到第四步； 对非叶节点 i，以 Di 为训练集，将特征集减去刚才的分裂特征(这里特别需要注意一点，如果该特征是类别特征，则在分裂完成后删除该特征；如果为连续特征，则不删除该特征)，以得到新的特征集，然后递归调用第一步至第三步。 总结 ID3树在选取分裂特征时采用信息增益原则(其实个人理解，无论是ID3、C4.5还是CART树，在选取分裂特征时，基于的总原则都是使分裂前后的数据复杂度减少程度最多，因为选取该特征才能使得这样构建出来的决策树分类或回归效果最好)。 ID3树后剪枝的策略是PEP(悲观误差剪枝)，这里后面再介绍，这里先占个坑哈哈哈。 C4.5树​ 上述ID3树采用的是信息增益来进行节点分裂特征的选取。信息增益原则对于每个分支节点，都会乘以其权重，也就是说，由于权重之和为1，所以分支节点分的越多，即每个节点数据越少，纯度可能越高。这样会导致信息增益准则偏爱那些取值数目较多的属性。 ​ 为了解决该问题，引入了信息增益率，这个也就是C4.5树的分裂准则。C4.5树的基本构建流程与ID3树类似，唯一的区别就是C4.5树采用信息增益率进行特征选择，而ID3树采用信息增益进行特征选择。 ​ 信息增益率的定义如下： G_{ratio}=\frac{g(D|A_m)}{H_{A_m}(D)}​ 其中， H_{A_m}(D)=-\sum_{i=1}^{K}\frac{|D_i|}{|D|}log2{\frac{|D_i|}{|D|}}​ 需要注意的是，信息增益率原则可能对取值数目较少的属性更加偏爱。因此，C4.5并不是直接选择增益最大的候选划分属性。为了解决这个问题，C4.5采用了一种启发式算法，可以先从候选划分属性中找出信息增益在平均值以上的属性，再从中选择信息增益率最高的。 CART树​ 这里先强调一点，CART树是二叉树。与ID3和C4.5的决策树所不同的是，ID3和C4.5生成的决策树可以是多叉的，每个节点下的叉树由该节点特征的取值种类而定，比如特征年龄分为(青年，中年，老年)，那么该节点下可分为3叉。而CART决策树为二叉树，内部节点特征取值为“是”和“否”。左分支取值为“是”，右分支取值为“否”。因此在搜索分裂特征时，是递归地二分每一个特征。个人理解为不仅要遍历每一个特征，还要遍历每一个特征的二分取值，等于是做了一次双层遍历。相比于ID3和C4.5要更加复杂一些。 ​ 另外，我们还需要注意一点，ID3和C4.5只能用于分类，而CART树既可以用于分类，也能用于回归。 ​ CART树的基本构建流程与ID3树和C4.5树类似。唯一的区别就在于特征选择准则不同，另外CART树不仅要选择最优特征，还要选择最优二值切分点。别的都一样哈哈哈！！ ​ CART分类树特征选择​ CART分类树通过基尼指数选择最优特征，同时决定该特征的最优二值切分点，而ID3和C4.5直接选择最有特征，不用划分二值点。 ​ 这里先介绍一下基尼指数，我个人的理解是基尼指数和前面的信息熵的概念是一样的，都是表征数据集的不纯度，因此我们如何选择最优特征和最优二值切分点呢？也就是分裂后如何使基尼指数下降最大！ ​ 分类问题，假设有数据集D有K个类，样本点属于第k个类的概率为 pk，则基尼指数定义为 Gini(D)=1-\sum_{k=1}^{K}p_k^2​ 在特征A下，集合D的基尼指数定义为 Gini_A(D)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)​ 那么如何选择最优特征和最优二值切分点呢？就是使切分前后的不确定性下降最大！即切分前的基尼指数和在特征A下切分后的基尼指数之间的差越大，越应该为最优特征和最优二值切分点。同理也可以得到，在特征A下切分后的基尼指数越小，越应该为最优特征和最优二值切分点。 CART回归树特征选择​ 跟CART分类树大致相同，区别主要在以下几点： ​ 分裂准则：选择特征A以及对应的属性值s将当前节点分到两个区域 R1 和 R2 中， R_1={(x|x_is)}​ 如何选择呢？就是使上述两个区域的均方误差和最小！ ​ 最终预测值：最终节点值为该节点下所有数据的平均值。 剪枝​ 剪枝很好理解，它的目的就是防止过拟合。它主要分为预剪枝和后剪枝。 预剪枝 通过提前停止树的构建而对树剪枝。一旦停止，节点就是叶节点。 停止决策树生长最简单的方法有： 定义一个高度，当决策树达到该高度时就停止决策树的生长； 当节点实例个数小于某一阈值； 当分裂增益小于某一阈值。 后剪枝 ​ 它首先构造完整的决策树，允许树过度拟合训练数据，然后对那些置信度不够的结点子树用叶子结点来代替，该叶子的类标号用该结点子树中最频繁的类标记。相比于先剪枝，这种方法更常用，正是因为在先剪枝方法中精确地估计何时停止树增长很困难。 ​ 后剪枝方法主要有以下几个方法： ​ Reduced-Error Pruning(REP，错误率降低剪枝） ​ Pessimistic-Error Pruning(PEP，悲观错误剪枝） ​ Cost-Complexity Pruning（CCP，代价复杂度剪枝) REP REP方法是一种最简单的后剪枝方法，对每个非叶子节点，从下往上遍历，尝试将该子树从叶子节点代替。然后利用验证集进行验证，看精度是否提升。若提升，则剪枝。这种方法一般不采用。 PEP PEP的策略主要用于ID3和C4.5树。悲观错误剪枝法是根据剪枝前后的错误率来判定子树的修剪。 把一颗子树（具有多个叶子节点）的分类用一个叶子节点来替代的话，在训练集上的误判率肯定是上升的，但是在新数据上不一定。于是我们需要把子树的误判计算加上一个经验性的惩罚因子。 对一个叶子节点，有n个样本，其中e个错误，则该叶子节点错误率为 (e+0.5)/n其中，0.5为惩罚因子 对一棵子树，有L个叶子节点，则该子树的错误率为 err\_ratio=\frac{(\sum_{i=1}^{L}e_i)+0.5L}{\sum_{i=1}^{L}n_i}这样的话，我们可以看到一颗子树虽然具有多个子节点，但由于加上了惩罚因子，所以子树的误判率计算未必占到便宜。 我们可以得到子树的错误次数均值和标准差 err\_mean=err\_ratio*\sum_{i=1}^{L}n_i err\_std=\sqrt{err\_ratio*\sum_{i=1}^{L}n_i*(1-err\_ratio)}当把子树替换成叶子节点后，该叶子节点的错误率和错误次数均值为 err\_ratio^{'}=\frac{(\sum_{i=1}^{L}e_i)+0.5}{\sum_{i=1}^{L}n_i} err\_mean^{'}=err\_ratio^{'}*\sum_{i=1}^{L}n_i剪枝条件为： err\_mean+err\_std>=err\_mean^{'}PEP的缺点： PEP算法采用的是自上而下的剪枝策略，这种剪枝会导致和预剪枝同样的问题，即剪枝过度。 CCP CCP的策略主要用于CART树。 代价指在剪枝过程中因子树 Tt 叶节点替代而增加的错分样本，复杂度表示剪枝后子树 Tt 减少的叶结点数，则定义每个非叶节点剪枝后树的复杂度降低程度与代价间的关系： α(t)=\frac{C(t)-C(T_t)}{|T_t|-1}其中， C(t) = r(t) * p(t)代表剪枝后叶子节点的误差代价 对于分类树，r(t) 为该节点的错误率；对于回归树，代表该节点的均方误差。 p(t) 为该节点数据占总数据的比例。 C(T_t)=\sum_{i=1}^{m}r_i(t)p_i(t)代表剪枝前的子树误差代价 对于分类树，ri(t) 为该子树某叶子节点的错误率；对于回归树，代表该子树某叶子节点的均方误差。 pi(t) 为该子树某叶子节点占总数据的比例。 CCP剪枝算法分为两个步骤： 对于完全决策树 T 自下而上的计算每个非叶结点的 α 值，然后自上而下的循环(注意，每剪枝一次，就重新计算表面误差增益率，再循环第一步)剪掉具有最小 α 值的子树(自上而下代表若有多个非叶子节点的表面误差率增益值相同小，则选择非叶子节点中子节点数最多的非叶子节点进行剪枝)，直到剩下根节点。在该步可得到一系列的剪枝树序列｛T0，T1，T2……Tn｝,其中 T0 为原有的完全决策树，Tn为根结点，Ti+1为对 Ti 进行剪枝的结果； 从子树序列中，根据真实的验证集误差选择最佳剪枝后决策树。 例. 下图是一棵子树，设决策树的总数据量为40 C(t)=\frac{8}{18}*\frac{18}{40}=\frac{1}{5} C(T_t)=\frac{1}{3}*\frac{3}{40}+\frac{4}{9}*\frac{9}{40}+\frac{1}{6}*\frac{6}{40}=\frac{6}{40} α(t)=\frac{\frac{1}{5}-\frac{6}{40}}{3-1}=\frac{1}{40}​ 好啦，到了这里，基本就把所有的决策树知识说完啦！希望能对大家有帮助。]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用hexo框架搭建个人博客(手把手教学)]]></title>
    <url>%2F2019%2F07%2F02%2F%E5%88%A9%E7%94%A8hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2(%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E5%AD%A6)%2F</url>
    <content type="text"><![CDATA[前言​ 本文是在Windows 10环境下搭建个人博客，对于Linux和Mac环境，搭建工作基本类似，区别无非在于个别命令行指令的区别，读者只需作相应的调整即可。 1. 安装NodeJs​ hexo是基于NodeJs环境的静态博客，里面的npm工具很有用（个人感觉类似于python里面的pip或者anaconda里面的conda，是一个包管理工具），所以说NodeJs是搭建个人博客的第一步。 下载地址：https://nodejs.org/en/ 下载下图的10.16.0 LTS即可。 然后点击下载的安装包，无脑点yes安装即可。至此NodeJs已经全部安装完成。 2. 安装Git Bash​ 由于本人使用的是Windows环境，cmd指令过于难用（也难怪众多开发者投入了Linux的怀抱）。所以本人下载了Git Bash（其实也就是下载了Git工具）。在网上搜索Git可以下载一个下面这个应用程序： ​ 然后依旧无脑点yes，安装完成后，在左面点击鼠标右键，如果出现了Git Bash和Git GUI即代表Git安装成功。 ​ 然后打开Git Bash可以查看NodeJs和npm的版本： NodeJs版本 1$ node -v npm版本 1$ npm -v 3. 安装hexo​ 我们是需要借助npm来安装hexo框架的。但是由于墙的原因，国内使用npm的下载速度很慢。因此我们需要更换源（使用淘宝的那个源），利用npm来安装一个cnpm。安装完成后，后面完全可以用cnpm全面取代npm。 ​ 如何安装cnpm？ 1$ npm install -g cnpm --registry=https://registry.npm.taobao.org ​ 安装完成后，可以查看一下cnpm的版本 1$ cnpm -v ​ 随后，开始正式安装hexo框架 1$ cnpm install -g hexo-cli ​ 如何判断hexo是否安装成功，我们只需要查看hexo的版本，如果出现版本信息，则代表hexo安装成功 1$ hexo -v 4. 开始正式搭建博客​ 首先我们需要先建立一个空的目录，存储我们的博客信息。这样的好处就是，一旦这个博客出错了或者你不想要了，直接把这个文件夹删除即可，不会影响其他的文件，保持独立性。 12$ mkdir blog$ cd blog ​ 随后在blog目录下利用hexo生成博客，很简单 1$ hexo init ​ 可以看到，生成完成后会出现下面这些文件 ​ 随后启动博客 1$ hexo s ​ 可以看到，博客可以在本地的4000端口进行访问 ​ 我们利用浏览器打开该端口，即可访问到我们最初始的博客。该博客默认有一篇hello word的博文 ​ 随后我们可以新建一篇文章，里面写上我们想要的内容即可。如何新建一篇文章？ 1$ hexo new &quot;我的第一篇文章&quot; ​ 生成完成后，便可以在blog/source/_post目录下找到我们生成的这篇文章，文件名为”我的第一篇文章.md”。随后便可以用Typora来写该博客了（markdown语法）。 ​ 因为我们现在新建了一篇文章，如何把它上传呢？（即在浏览器中也可以看到改文章）。这里就要用到hexo中著名的“素质三连”指令了 123$ hexo clean (清理)$ hexo g (生成)$ hexo s (启动) ​ 随后便可以在本地4000端口看见我们刚才新建的那篇文章了。 一切都是这么简单，一切都是这么优雅！ 5. 远端部署​ 我们写个人博客的目的不是为了只自己访问，也希望别人能看见，因此需要部署到远端服务器。这里有一种免费的部署方式，即把我们的博客部署到github上公开使用。 5.1 新建仓库 ​ 一定要注意仓库的命名方式，仓库的名称一定要和你的github昵称保持一致，否则将无法访问。例如我的github昵称为pringce，那么仓库的名称一定要命名为pringce.github.io。如下图 5.2 安装Git部署插件1$ cnpm install hexo-deployer-git --save 5.3 配置config文件​ 在我们的blog目录下，找到_config.yml这个文件，然后到文件的最底部，写入如下配置 5.4 部署到远端1$ hexo d ​ 随后输入”https://pringce.github.io&quot;，即可访问我们部署到远端服务器上的博客了。至此任何人都可以访问你的博客了，博客的部署工作完成。 一切都是这么简单，一切都是这么优雅！]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>个人博客</tag>
      </tags>
  </entry>
</search>
