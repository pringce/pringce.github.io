---
title: 决策树全解析
date: 2019-07-20 18:00:21
categories:
- 机器学习算法
tags:
- 机器学习
- 分类算法
mathjax: true
---

# 前言

​		其实大家都说决策树简单，但个人觉得如果想学好这个算法其实还是需要下一些功夫的。决策树是一种基本的分类与回归方法(不要觉得决策树只能分类哦，它也可以做回归的)。

​		为了方便下面的讨论，我们先做一下说明：

- ​		假设有一组训练数据
  $$
  D=(x1,y1),(x2,y2),\cdots,(xn,yn)
  $$
  ​		表示有n个样本，样本共分为K类情况下，yi的取值来自K个类别值
  $$
  (C1,C2,\cdots,Ck)
  $$



​		另外，无论是ID3树、C4.5树还是CART树都是差不多的，都由**特征选择(分裂特征选择算法)**、**树的生成**和**剪枝**组成。它们之间的区别主要是特征选择和剪枝算法不同，这些我们后面会详细讨论，这里先留个伏笔。

# ID3树

​		在介绍ID3决策树之前，我想先说一个概念：**信息增益**。

​		一个离散型随机变量x的概率分布为
$$
P(x=x_i)=p_i
$$
​		，则x的信息熵定义为
$$
H(x)=-\sum_{i=1}^{n}{p_i}
$$
​		**数据集的熵**表征其类别的不确定程度，而数据集关于某个特征的**条件熵**则表征着给定某个特征后，其类别的不确定程度：

- **数据集的熵—条件熵 = 该特征使数据集不确定性减小程度**

  设数据集D的熵为**H(D)**，关于特征Am的条件熵为**H(D|Am)**，因此可以获得**信息增益**为
  $$
  g(D|A_m)=H(D)-H(D|A_m)
  $$
  其中，
  $$
  H(D)=-\sum_{k=1}^{K}{\frac{|C_k|}{|D|}}log_2\frac{|C_k|}{|D_k|}
  $$

$$
H(D|A_m)=\sum_{i=1}^{K}{\frac{|D_i|}{|D|}}H(D_i)
$$

​		**Di** 表示训练样本中特征A取值为 **ai** 的样本点集合(比如说特征A有三个可能的取值，则K为3)。

## ID3决策树构建

​		自根节点开始，选择信息增益最大的特征作为节点的分类特征，并根据该特征的可能取值将训练数据分配到不同的子节点(对子节点进行同样的操作)。

​		若子节点的所有样本属于同一类别或该子节点处所有特征的信息增益均小于给定阈值或无可供选择的特征，此时我们判定该子节点为一个叶节点，将该叶节点的样本数量最多的类别作为该叶节点的类别。

​		算法步骤如下：

1. 若D中样本特征为空，那么树T为一棵单节点树，将样本数量最大的类别作为节点类别，返回T。否则转到第二步；
2. 计算D关于所有特征的信息增益，若信息增益均小于某一阈值，则T为单节点树。否则转到第三步；
3. 选择信息增益最大的特征作为分裂特征，依据该分裂特征的所有可能取值建立相应子节点。若子节点样本全为同一类别，则子节点为叶节点。若均为叶节点，返回T。否则转到第四步；
4. 对非叶节点 i，以 **Di** 为训练集，将特征集减去刚才的分裂特征(**这里特别需要注意一点，如果该特征是类别特征，则在分裂完成后删除该特征；如果为连续特征，则不删除该特征**)，以得到新的特征集，然后递归调用第一步至第三步。

## 总结

- ID3树在选取分裂特征时采用**信息增益**原则(其实个人理解，无论是ID3、C4.5还是CART树，在选取分裂特征时，基于的总原则都是使分裂前后的数据复杂度减少程度最多，因为选取该特征才能使得这样构建出来的决策树分类或回归效果最好)。

- ID3树后剪枝的策略是**PEP**(悲观误差剪枝)，这里后面再介绍，这里先占个坑哈哈哈。


# C4.5树

​		上述ID3树采用的是信息增益来进行节点分裂特征的选取。信息增益原则对于每个分支节点，都会乘以其权重，也就是说，由于权重之和为1，所以分支节点分的越多，即每个节点数据越少，纯度可能越高。这样会导致**信息增益准则偏爱那些取值数目较多的属性**。

​		为了解决该问题，引入了**信息增益率**，这个也就是C4.5树的分裂准则。**C4.5树的基本构建流程与ID3树类似**，唯一的区别就是C4.5树采用信息增益率进行特征选择，而ID3树采用信息增益进行特征选择。

​		信息增益率的定义如下：
$$
G_{ratio}=\frac{g(D|A_m)}{H_{A_m}(D)}
$$
​		其中，
$$
H_{A_m}(D)=-\sum_{i=1}^{K}\frac{|D_i|}{|D|}log2{\frac{|D_i|}{|D|}}
$$
​		需要注意的是，**信息增益率原则可能对取值数目较少的属性更加偏爱**。因此，C4.5并不是直接选择增益最大的候选划分属性。为了解决这个问题，C4.5采用了一种**启发式算法**，可以**先从候选划分属性中找出信息增益在平均值以上的属性，再从中选择信息增益率最高的**。



# CART树

​		这里先强调一点，**CART树是二叉树**。与ID3和C4.5的决策树所不同的是，**ID3和C4.5生成的决策树可以是多叉的，每个节点下的叉树由该节点特征的取值种类而定，比如特征年龄分为(青年，中年，老年)，那么该节点下可分为3叉。而CART决策树为二叉树，内部节点特征取值为“是”和“否”。左分支取值为“是”，右分支取值为“否”**。因此在搜索分裂特征时，是递归地二分每一个特征。个人理解为**不仅要遍历每一个特征，还要遍历每一个特征的二分取值**，等于是做了一次**双层遍历**。相比于ID3和C4.5要更加复杂一些。

​		另外，我们还需要注意一点，**ID3和C4.5只能用于分类**，而**CART树既可以用于分类**，也能用于回归。

​		**CART树的基本构建流程与ID3树和C4.5树类似。**唯一的区别就在于特征选择准则不同，另外CART树不仅要选择最优特征，还要选择最优二值切分点。别的都一样哈哈哈！！

​		

## CART分类树特征选择

​		CART分类树通过**基尼指数**选择最优特征，同时决定该特征的最优二值切分点，而ID3和C4.5直接选择最有特征，不用划分二值点。

​		这里先介绍一下基尼指数，我个人的理解是基尼指数和前面的信息熵的概念是一样的，都是表征数据集的不纯度，因此我们如何选择最优特征和最优二值切分点呢？也就是分裂后**如何使基尼指数下降最大！**

​		分类问题，假设有数据集D有K个类，样本点属于第k个类的概率为 **pk**，则**基尼指数**定义为
$$
Gini(D)=1-\sum_{k=1}^{K}p_k^2
$$
​		在特征A下，集合D的基尼指数定义为
$$
Gini_A(D)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$
​		那么如何选择**最优特征**和**最优二值切分点**呢？就是使切分前后的不确定性下降最大！即**切分前的基尼指数**和**在特征A下切分后的基尼指数**之间的差越大，越应该为最优特征和最优二值切分点。同理也可以得到，在特征A下切分后的基尼指数越小，越应该为最优特征和最优二值切分点。

## CART回归树特征选择

​		跟CART分类树大致相同，区别主要在以下几点：

- ​	**分裂准则**：选择特征A以及对应的属性值s将当前节点分到两个区域 **R1** 和 **R2** 中，
  $$
  R_1={(x|x_i<=s),R_2=(x|x_i>s)}
  $$
  ​	如何选择呢？**就是使上述两个区域的均方误差和最小！**

- ​    **最终预测值**：最终节点值为该节点下所有数据的平均值。

# 剪枝

​		剪枝很好理解，它的目的就是**防止过拟合**。它主要分为**预剪枝**和**后剪枝**。

- 预剪枝

  通过提前停止树的构建而对树剪枝。一旦停止，节点就是叶节点。

  停止决策树生长最简单的方法有：

  1. 定义一个高度，当决策树达到该高度时就停止决策树的生长；
  2. 当节点实例个数小于某一阈值；
  3. 当分裂增益小于某一阈值。

- 后剪枝

​        它首先构造完整的决策树，允许树过度拟合训练数据，然后对那些置信度不够的结点子树用叶子结点来代替，该叶子的类标号用该结点子树中最频繁的类标记。相比于先剪枝，这种方法更常用，正是因为在先剪枝方法中精确地估计何时停止树增长很困难。

​		后剪枝方法主要有以下几个方法：

​		**Reduced-Error Pruning(REP，错误率降低剪枝）**

​		**Pessimistic-Error Pruning(PEP，悲观错误剪枝）**

​		**Cost-Complexity Pruning（CCP，代价复杂度剪枝)**



1. **REP**

   REP方法是一种最简单的后剪枝方法，对每个非叶子节点，**从下往上遍历**，尝试将该子树从叶子节点代替。然后利用验证集进行验证，看精度是否提升。若提升，则剪枝。这种方法一般不采用。

2. **PEP**

   PEP的策略主要用于**ID3**和**C4.5**树。**悲观错误剪枝法是根据剪枝前后的错误率来判定子树的修剪。**

   **把一颗子树（具有多个叶子节点）的分类用一个叶子节点来替代的话，在训练集上的误判率肯定是上升的，但是在新数据上不一定。于是我们需要把子树的误判计算加上一个经验性的惩罚因子。**

   对一个叶子节点，有n个样本，其中e个错误，则该叶子节点错误率为
   $$
   (e+0.5)/n 
   $$
   其中，0.5为惩罚因子

   对一棵子树，有L个叶子节点，则该子树的错误率为
   $$
   err\_ratio=\frac{(\sum_{i=1}^{L}e_i)+0.5L}{\sum_{i=1}^{L}n_i}
   $$
   **这样的话，我们可以看到一颗子树虽然具有多个子节点，但由于加上了惩罚因子，所以子树的误判率计算未必占到便宜。**

   我们可以得到子树的错误次数均值和标准差
   $$
   err\_mean=err\_ratio*\sum_{i=1}^{L}n_i
   $$

   $$
   err\_std=\sqrt{err\_ratio*\sum_{i=1}^{L}n_i*(1-err\_ratio)}
   $$

   当把子树替换成叶子节点后，该叶子节点的错误率和错误次数均值为
   $$
   err\_ratio^{'}=\frac{(\sum_{i=1}^{L}e_i)+0.5}{\sum_{i=1}^{L}n_i}
   $$

   $$
   err\_mean^{'}=err\_ratio^{'}*\sum_{i=1}^{L}n_i
   $$

   **剪枝条件为**：
   $$
   err\_mean+err\_std>=err\_mean^{'}
   $$
   **PEP的缺点**：

   PEP算法采用的是**自上而下**的剪枝策略，这种剪枝会导致和预剪枝同样的问题，即**剪枝过度**。

3. **CCP**

   CCP的策略主要用于CART树。

   代价指在剪枝过程中因子树 **Tt** 叶节点替代而增加的错分样本，复杂度表示剪枝后子树 **Tt** 减少的叶结点数，则定义每个非叶节点剪枝后树的复杂度降低程度与代价间的关系：
   $$
   α(t)=\frac{C(t)-C(T_t)}{|T_t|-1}
   $$
   其中，
   $$
   C(t) = r(t) * p(t)
   $$
   代表剪枝后叶子节点的误差代价

   对于**分类树**，**r(t)** 为该节点的错误率；对于**回归树**，代表该节点的均方误差。

   **p(t)** 为该节点数据占总数据的比例。
   $$
   C(T_t)=\sum_{i=1}^{m}r_i(t)p_i(t)
   $$
   代表剪枝前的子树误差代价

   对于**分类树**，**ri(t)** 为该子树某叶子节点的错误率；对于**回归树**，代表该子树某叶子节点的均方误差。

   **pi(t)** 为该子树某叶子节点占总数据的比例。

   

   **CCP剪枝算法分为两个步骤：**

   1. 对于完全决策树 T **自下而上**的计算每个非叶结点的 α 值，然后**自上而下**的循环(**注意，每剪枝一次，就重新计算表面误差增益率，再循环第一步**)剪掉具有最小 α 值的子树(**自上而下代表若有多个非叶子节点的表面误差率增益值相同小，则选择非叶子节点中子节点数最多的非叶子节点进行剪枝**)，直到剩下根节点。在该步可得到一系列的剪枝树序列**｛T0，T1，T2......Tn｝**,其中 **T0** 为原有的完全决策树，**Tn**为根结点，**Ti+1**为对 **Ti** 进行剪枝的结果；
   2. 从子树序列中，根据真实的验证集误差选择最佳剪枝后决策树。

   

   **例.** 下图是一棵子树，设决策树的总数据量为40

   {% asset_img  20190720162435.png %}
   $$
   C(t)=\frac{8}{18}*\frac{18}{40}=\frac{1}{5}
   $$

   $$
   C(T_t)=\frac{1}{3}*\frac{3}{40}+\frac{4}{9}*\frac{9}{40}+\frac{1}{6}*\frac{6}{40}=\frac{6}{40}
   $$

   $$
   α(t)=\frac{\frac{1}{5}-\frac{6}{40}}{3-1}=\frac{1}{40}
   $$




​		好啦，到了这里，基本就把所有的决策树知识说完啦！希望能对大家有帮助。